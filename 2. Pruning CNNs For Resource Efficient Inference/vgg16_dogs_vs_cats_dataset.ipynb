{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from heapq import nsmallest\n",
    "from operator import itemgetter\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import argparse\n",
    "import cv2\n",
    "import glob\n",
    "import ipdb\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model and pruner class\n",
    "class ModifiedVGG16Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        model = models.vgg16(pretrained=True) \n",
    "        self.features = model.features #use the pre-trained feature head\n",
    "\n",
    "        for param in self.features.parameters(): #freeze the feature head\n",
    "            param.requires_grad = False \n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(25088, 4096), #output of last conv is 7x7x512 (25088). Feed that to FC layer \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2)) #final classifer is 2 category\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) #x.shape torch.Size([32, 512, 7, 7]) (batchsize 32)\n",
    "        x = x.view(x.size(0), -1) #x.shape torch.Size([32, 25088])\n",
    "        x = self.classifier(x) #x.shape torch.Size([32, 2])\n",
    "        return x\n",
    "\n",
    "class FilterPrunner:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.filter_ranks = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        self.grad_index = 0 #only used to find current activation_index in self.compute_rank\n",
    "        self.activation_to_layer = {}\n",
    "\n",
    "        activation_index = 0\n",
    "        #first pass x through feature head\n",
    "        for layer, (name, module) in enumerate(self.model.features._modules.items()):\n",
    "            x = module(x)\n",
    "            if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "                x.register_hook(self.compute_rank) \n",
    "                #when grad of x is computed, self.compute_rank is called with grad of x as arg\n",
    "                self.activations.append(x)\n",
    "                self.activation_to_layer[activation_index] = layer\n",
    "                activation_index += 1\n",
    "        #pass x through classifier head\n",
    "        return self.model.classifier(x.view(x.size(0), -1))\n",
    "\n",
    "    def compute_rank(self, grad):\n",
    "        activation_index = len(self.activations) - self.grad_index - 1\n",
    "        activation = self.activations[activation_index] # get current activation\n",
    "\n",
    "        taylor = activation * grad #grad is arg variable. activation is extracted (saved in forward pass) \n",
    "        # Get the average value for every filter, \n",
    "        # accross all the other dimensions\n",
    "        taylor = taylor.mean(dim=(0, 2, 3)).data # sum in all dimensions except the dimension of the output\n",
    "\n",
    "\n",
    "        if activation_index not in self.filter_ranks:\n",
    "            self.filter_ranks[activation_index] =  torch.FloatTensor(activation.size(1)).zero_()\n",
    "            self.filter_ranks[activation_index] = self.filter_ranks[activation_index].cuda()\n",
    "\n",
    "        self.filter_ranks[activation_index] += taylor\n",
    "        self.grad_index += 1\n",
    "\n",
    "    def lowest_ranking_filters(self, num):\n",
    "        data = []\n",
    "        for i in sorted(self.filter_ranks.keys()):\n",
    "            for j in range(self.filter_ranks[i].size(0)):\n",
    "                data.append((self.activation_to_layer[i], j, self.filter_ranks[i][j]))\n",
    "\n",
    "        return nsmallest(num, data, itemgetter(2))\n",
    "\n",
    "    def normalize_ranks_per_layer(self):\n",
    "        '''\n",
    "        self.filter_ranks is a dict. \n",
    "        key is conv filter number. value is a 1D vector (tensor) (taylor scores) of size same as  number of filters in conv layer. \n",
    "        self.filter_ranks.keys=dict_keys([12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
    "        each key represents a conv layer\n",
    "        0 is earliest layer of 64 filters\n",
    "        12 is deepest layer of 512 filters\n",
    "        below list of conv layer number, and number of filters in each conv later. \n",
    "        [(0,64),(1,64),(2,128),(3,128),(4,256),(5,256),(6,256),(7,512),(8,512),(9,512),(10,512),(11,512),(12,512)]\n",
    "        '''\n",
    "        \n",
    "        for i in self.filter_ranks:\n",
    "            v = torch.abs(self.filter_ranks[i])\n",
    "            #v = v / np.sqrt(torch.sum(v * v))\n",
    "            v = v / torch.sqrt(torch.sum(v * v))\n",
    "            self.filter_ranks[i] = v.cpu()  #v (or self.filter_ranks[i] was originally on cuda. check why)\n",
    "            #what if you dont convert to cpu here\n",
    "            #self.filter_ranks[i] = v #verify later remving .cpu() has what implications\n",
    "\n",
    "    def get_prunning_plan(self, num_filters_to_prune):\n",
    "        filters_to_prune = self.lowest_ranking_filters(num_filters_to_prune)\n",
    "        '''\n",
    "        filters_to_prune is a list of 3-tuples\n",
    "        [(layer_number,filter_number,tensor_number)]\n",
    "        len(filters_to_prune)=32 (filters removed per iteration)\n",
    "        [(21, 389, tensor(2.0786e-06)), (26, 229, tensor(9.5175e-06)), (26, 480, tensor(2.4851e-05))...]\n",
    "        '''\n",
    "        \n",
    "        # After each of the k filters are prunned,\n",
    "        # the filter index of the next filters change since the model is smaller.\n",
    "        filters_to_prune_per_layer = {}\n",
    "        for (l, f, _) in filters_to_prune: #change with default dict\n",
    "            if l not in filters_to_prune_per_layer:\n",
    "                filters_to_prune_per_layer[l] = []\n",
    "            filters_to_prune_per_layer[l].append(f)\n",
    "        '''\n",
    "        {28: [358, 447, 147, 472, 143], 24: [356, 216, 250, 437, 182], 17: [3, 92, 277, 408, 477, 301, 360], \n",
    "        19: [210, 38, 262, 285], 14: [116, 112], 21: [428, 11, 430, 15], 26: [415, 91, 93, 167], 10: [116]}\n",
    "        '''\n",
    "        for l in filters_to_prune_per_layer:\n",
    "            filters_to_prune_per_layer[l] = sorted(filters_to_prune_per_layer[l]) #sorts the filter order in indivisual conv layer\n",
    "            for i in range(len(filters_to_prune_per_layer[l])): \n",
    "                filters_to_prune_per_layer[l][i] = filters_to_prune_per_layer[l][i] - i #you will prune one filter at a time. once you prune a filter, the filter number of remainig filters shifts \n",
    "        '''\n",
    "        {28: [143, 146, 356, 444, 468], 24: [182, 215, 248, 353, 433], 17: [3, 91, 275, 298, 356, 403, 471], \n",
    "        19: [38, 209, 260, 282], 14: [112, 115], 21: [11, 14, 426, 427], 26: [91, 92, 165, 412], 10: [116]}\n",
    "        '''\n",
    "        filters_to_prune = []\n",
    "        for l in filters_to_prune_per_layer:\n",
    "            for i in filters_to_prune_per_layer[l]:\n",
    "                filters_to_prune.append((l, i))\n",
    "        '''\n",
    "        \n",
    "        filters_to_prune_per_layer is ->\n",
    "        {28: [135, 155, 177, 214, 229, 249, 402, 491], 26: [88], 21: [37, 225, 301, 305], \n",
    "        24: [83, 137, 191, 197, 236, 289, 426], 5: [64], 17: [9, 145, 151, 438], 7: [41], \n",
    "        19: [50, 402, 411, 491], 14: [116], 12: [85]}\n",
    "        \n",
    "        filters_to_prune is -> (32)\n",
    "        [(28, 135), (28, 155), (28, 177), (28, 214), (28, 229), (28, 249), (28, 402), (28, 491), \n",
    "        (26, 88), (21, 37), (21, 225), (21, 301), (21, 305), (24, 83), (24, 137), (24, 191), (24, 197), \n",
    "        (24, 236), (24, 289), (24, 426), (5, 64), (17, 9), (17, 145), (17, 151), (17, 438), (7, 41), \n",
    "        (19, 50), (19, 402), (19, 411), (19, 491), (14, 116), (12, 85)]\n",
    "        '''\n",
    "        return filters_to_prune             \n",
    "\n",
    "class PrunningFineTuner_VGG16:\n",
    "    def __init__(self, train_path, test_path, model):\n",
    "        self.train_data_loader = loader(train_path)\n",
    "        self.test_data_loader = test_loader(test_path)\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.prunner = FilterPrunner(self.model) \n",
    "        self.model.train()\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, (batch, label) in enumerate(self.test_data_loader):\n",
    "            batch = batch.cuda()\n",
    "            output = model(Variable(batch))\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.cpu().eq(label).sum()\n",
    "            total += label.size(0)\n",
    "        \n",
    "        print(\"Accuracy :\", float(correct) / total)\n",
    "        \n",
    "        self.model.train()\n",
    "\n",
    "    def train(self, optimizer = None, epoches=10):\n",
    "        if optimizer is None:\n",
    "            optimizer = optim.SGD(model.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "        for i in range(epoches):\n",
    "            print(\"Epoch: \", i)\n",
    "            self.train_epoch(optimizer)\n",
    "            self.test()\n",
    "        print(\"Finished fine tuning.\")\n",
    "        \n",
    "\n",
    "    def train_batch(self, optimizer, batch, label, rank_filters):\n",
    "\n",
    "        batch = batch.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        input = Variable(batch)\n",
    "\n",
    "        if rank_filters:\n",
    "            output = self.prunner.forward(input)\n",
    "            self.criterion(output, Variable(label)).backward()\n",
    "        else:\n",
    "            self.criterion(self.model(input), Variable(label)).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def train_epoch(self, optimizer = None, rank_filters = False):\n",
    "        for i, (batch, label) in enumerate(self.train_data_loader):\n",
    "            self.train_batch(optimizer, batch, label, rank_filters)\n",
    "\n",
    "    def get_candidates_to_prune(self, num_filters_to_prune):\n",
    "        self.prunner.reset() #filter_ranks = {}\n",
    "        self.train_epoch(rank_filters = True) #do a forward and backward pass to rank the filters\n",
    "        self.prunner.normalize_ranks_per_layer()\n",
    "        return self.prunner.get_prunning_plan(num_filters_to_prune)\n",
    "        \n",
    "    def total_num_filters(self):\n",
    "        filters = 0\n",
    "        for name, module in self.model.features._modules.items():\n",
    "            if isinstance(module, torch.nn.modules.conv.Conv2d):\n",
    "                filters = filters + module.out_channels\n",
    "        return filters\n",
    "\n",
    "    def prune(self,num_filters_to_prune_per_iteration=512,percentage_to_prune=67):\n",
    "        self.test() #Get the accuracy before prunning\n",
    "        self.model.train()\n",
    "\n",
    "        #Make sure all the layers are trainable\n",
    "        for param in self.model.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        number_of_filters = self.total_num_filters() #total filters in conv layers in the NN before pruning. 4224\n",
    "        iterations = int(float(number_of_filters) / num_filters_to_prune_per_iteration) #8\n",
    "\n",
    "        iterations = int(iterations * percentage_to_prune * 0.01) #5\n",
    "\n",
    "        print(\"Number of prunning iterations to remove \"+ str(percentage_to_prune) +\"% filters : \", iterations)\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            print(\"Ranking filters.. \")\n",
    "            prune_targets = self.get_candidates_to_prune(num_filters_to_prune_per_iteration)\n",
    "            '''\n",
    "            prune_targets is list of tuples -> \n",
    "            [(28, 66), (28, 100), (28, 151), (28, 285), (28, 305), (28, 321), (28, 346), (28, 357), (26, 67), \n",
    "            (26, 84), (26, 89), (26, 223), (26, 224), (26, 224), (26, 432), (17, 41), (17, 117), (17, 258), \n",
    "            (14, 18), (14, 47), (24, 52), (24, 52), (12, 160), (19, 0), (19, 189), (19, 209), (19, 334), \n",
    "            (19, 457), (2, 3), (21, 496), (10, 36), (10, 53)]\n",
    "            '''\n",
    "            layers_prunned = {} #just for printing below. no real use\n",
    "            for layer_index, filter_index in prune_targets: #better to use default dict\n",
    "                if layer_index not in layers_prunned:\n",
    "                    layers_prunned[layer_index] = 0\n",
    "                layers_prunned[layer_index] = layers_prunned[layer_index] + 1 \n",
    "            '''\n",
    "            layers_prunned - {17: 7, 28: 8, 26: 6, 21: 5, 19: 1, 24: 2, 14: 2, 7: 1}\n",
    "            '''\n",
    "            print(\"Layer number : number of filters in that layer that will be prunned\", layers_prunned)\n",
    "        \n",
    "            print(\"Prunning filters.. \")\n",
    "            model = self.model.cpu()\n",
    "            for layer_index, filter_index in prune_targets:\n",
    "                model = prune_vgg16_conv_layer(model, layer_index, filter_index, use_cuda=True)\n",
    "\n",
    "            self.model = model\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "            message = str(100*float(self.total_num_filters()) / number_of_filters) + \"%\"\n",
    "            print(\"Filters prunned\", str(message))\n",
    "            self.test()\n",
    "            print(\"Fine tuning to recover from prunning iteration.\")\n",
    "            optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n",
    "            self.train(optimizer, epoches = 10)\n",
    "\n",
    "\n",
    "        print(\"Finished. Going to fine tune the model a bit more\")\n",
    "        self.train(optimizer, epoches=15)\n",
    "        #torch.save({'state_dict': model.state_dict()}, 'model_prunned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "        \n",
    "def replace_layers(model, i, indexes, layers):\n",
    "    if i in indexes:\n",
    "        return layers[indexes.index(i)]\n",
    "    return model[i]\n",
    "\n",
    "def prune_vgg16_conv_layer(model, layer_index, filter_index, use_cuda=False):\n",
    "    _, conv = list(model.features._modules.items())[layer_index] #pluck out the current conv layer\n",
    "    '''\n",
    "    print(list(model.features._modules.items())[layer_index])\n",
    "    > ('19', Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))\n",
    "\n",
    "    model.features._modules is an ordered dict of {conv layer number : conv layer object}\n",
    "    model.features._modules.items() gives a iterator over contents of dict\n",
    "    list(model.features._modules.items()) converts ordered dict into list of [(key,value)] ....\n",
    "    '''\n",
    "    next_conv = None\n",
    "    offset = 1 \n",
    "    while layer_index + offset <  len(model.features._modules.items()): #check  if layer number (layer_index + offset) would be a valid layer\n",
    "        res =  list(model.features._modules.items())[layer_index+offset] #pluck out one of the next layers\n",
    "        if isinstance(res[1], torch.nn.modules.conv.Conv2d): #check  if one of the next layers is a conv layer\n",
    "            next_name, next_conv = res #if so, save it and break the loop\n",
    "            break\n",
    "        offset = offset + 1\n",
    "    # above is done because there can be case of conv-relu-conv ; conv-relu-maxpool-conv; conv-batchnorm-relu-maxpool-conv \n",
    "    # many cases possible. makes the \"next conv\" search architecture agnostic\n",
    "    # next, create a new conv layer for current conv\n",
    "    new_conv = \\\n",
    "        torch.nn.Conv2d(in_channels = conv.in_channels, \\\n",
    "            out_channels = conv.out_channels - 1, #main change\n",
    "            kernel_size = conv.kernel_size, \\\n",
    "            stride = conv.stride,\n",
    "            padding = conv.padding,\n",
    "            dilation = conv.dilation,\n",
    "            groups = conv.groups,\n",
    "            bias = (conv.bias is not None)) #bias is boolean arg. use bias of original conv had bias\n",
    "\n",
    "    old_weights = conv.weight.data.cpu().numpy()\n",
    "    new_weights = new_conv.weight.data.cpu().numpy() #would be random to begin with. xavier init\n",
    "\n",
    "    new_weights[: filter_index, :, :, :] = old_weights[: filter_index, :, :, :]\n",
    "    new_weights[filter_index : , :, :, :] = old_weights[filter_index + 1 :, :, :, :] #skip filter_index weights in old_weights\n",
    "    #now that new_weights are made, save them to new_conv\n",
    "    new_conv.weight.data = torch.from_numpy(new_weights) \n",
    "    new_conv.weight.data = new_conv.weight.data.cuda() #hard code use cuda\n",
    "\n",
    "    bias_numpy = conv.bias.data.cpu().numpy() #old bias\n",
    "\n",
    "    bias = np.zeros(shape = (bias_numpy.shape[0] - 1), dtype = np.float32) #one less bias. initialize a vector of zeros of same size\n",
    "    bias[:filter_index] = bias_numpy[:filter_index]\n",
    "    bias[filter_index : ] = bias_numpy[filter_index + 1 :] #skip bias_numpy[filter_index]\n",
    "    new_conv.bias.data = torch.from_numpy(bias) #save to new_conv\n",
    "    new_conv.bias.data = new_conv.bias.data.cuda() #hardcode cuda\n",
    "\n",
    "    if not next_conv is None: #if next layer is a conv\n",
    "        #make a new conv with one less in_channels\n",
    "        next_new_conv = \\\n",
    "            torch.nn.Conv2d(in_channels = next_conv.in_channels - 1,\\\n",
    "                out_channels =  next_conv.out_channels, \\\n",
    "                kernel_size = next_conv.kernel_size, \\\n",
    "                stride = next_conv.stride,\n",
    "                padding = next_conv.padding,\n",
    "                dilation = next_conv.dilation,\n",
    "                groups = next_conv.groups,\n",
    "                bias = (next_conv.bias is not None))\n",
    "\n",
    "        old_weights = next_conv.weight.data.cpu().numpy()\n",
    "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, : filter_index, :, :] = old_weights[:, : filter_index, :, :]\n",
    "        new_weights[:, filter_index : , :, :] = old_weights[:, filter_index + 1 :, :, :]\n",
    "        next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "        next_new_conv.weight.data = next_new_conv.weight.data.cuda()\n",
    "\n",
    "        next_new_conv.bias.data = next_conv.bias.data  #no change is bias. copy over\n",
    "        \n",
    "    #if not next_conv is None: #redundant i think. \n",
    "        features_list=[replace_layers(model.features, i, [layer_index, layer_index+offset], \\\n",
    "                    [new_conv, next_new_conv]) for i, _ in enumerate(model.features)]\n",
    "        features = torch.nn.Sequential(*(features_list)) #stitch the network back together\n",
    "        del model.features #free memory\n",
    "        del conv\n",
    "\n",
    "        model.features = features\n",
    "\n",
    "    else:\n",
    "        #Prunning the last conv layer. This affects the first linear layer of the classifier.\n",
    "        # now model.features would have only 1 new conv layer to attach in\n",
    "        model.features = torch.nn.Sequential(\n",
    "                *(replace_layers(model.features, i, [layer_index], \\\n",
    "                    [new_conv]) for i, _ in enumerate(model.features)))\n",
    "        layer_index = 0\n",
    "        old_linear_layer = None\n",
    "        for _, module in model.classifier._modules.items():\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                old_linear_layer = module\n",
    "                break #find the first occurance of linear_layer and break\n",
    "            layer_index = layer_index  + 1\n",
    "\n",
    "        if old_linear_layer is None:\n",
    "            raise BaseException(\"No linear laye found in classifier\")\n",
    "        params_per_input_channel = old_linear_layer.in_features // conv.out_channels\n",
    "\n",
    "        new_linear_layer = \\\n",
    "            torch.nn.Linear(old_linear_layer.in_features - params_per_input_channel, \n",
    "                old_linear_layer.out_features)\n",
    "        \n",
    "        old_weights = old_linear_layer.weight.data.cpu().numpy()\n",
    "        new_weights = new_linear_layer.weight.data.cpu().numpy()        \n",
    "\n",
    "        new_weights[:, : filter_index * params_per_input_channel] = \\\n",
    "            old_weights[:, : filter_index * params_per_input_channel]\n",
    "        new_weights[:, filter_index * params_per_input_channel :] = \\\n",
    "            old_weights[:, (filter_index + 1) * params_per_input_channel :]\n",
    "        \n",
    "        new_linear_layer.bias.data = old_linear_layer.bias.data #bias remains same\n",
    "\n",
    "        new_linear_layer.weight.data = torch.from_numpy(new_weights) #save the new weights in FC layer object\n",
    "        new_linear_layer.weight.data = new_linear_layer.weight.data.cuda() #hard code cuda\n",
    "\n",
    "        classifier = torch.nn.Sequential(\n",
    "            *(replace_layers(model.classifier, i, [layer_index], \\\n",
    "                [new_linear_layer]) for i, _ in enumerate(model.classifier)))\n",
    "\n",
    "        del model.classifier\n",
    "        del next_conv\n",
    "        del conv\n",
    "        model.classifier = classifier\n",
    "\n",
    "    return model\n",
    "\n",
    "def loader(path, batch_size=32, num_workers=4, pin_memory=True):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return data.DataLoader(\n",
    "        datasets.ImageFolder(path,\n",
    "                             transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize,\n",
    "                             ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory)\n",
    "\n",
    "def test_loader(path, batch_size=32, num_workers=4, pin_memory=True):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return data.DataLoader(\n",
    "        datasets.ImageFolder(path,\n",
    "                             transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize,\n",
    "                             ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Accuracy : 0.9625\n",
      "Epoch:  1\n",
      "Accuracy : 0.97\n",
      "Finished fine tuning.\n"
     ]
    }
   ],
   "source": [
    "model = ModifiedVGG16Model()\n",
    "model = model.cuda()\n",
    "initial_training_obj = PrunningFineTuner_VGG16(\"train\", \"test\", model)\n",
    "initial_training_obj.train(epoches=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97\n"
     ]
    }
   ],
   "source": [
    "initial_training_obj.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'state_dict': model.state_dict()}, 'checkpoint_models/trained_model_state.pt') #save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97\n",
      "Accuracy : 0.97\n",
      "Number of prunning iterations to remove 20% filters :  1\n",
      "Ranking filters.. \n",
      "Layer number : number of filters in that layer that will be prunned {14: 11, 28: 216, 21: 38, 17: 35, 26: 88, 24: 48, 19: 34, 2: 2, 12: 11, 5: 5, 10: 15, 0: 3, 7: 6}\n",
      "Prunning filters.. \n",
      "Filters prunned 87.87878787878788%\n",
      "Accuracy : 0.9625\n",
      "Fine tuning to recover from prunning iteration.\n",
      "Epoch:  0\n",
      "Accuracy : 0.97625\n",
      "Epoch:  1\n",
      "Accuracy : 0.98\n",
      "Epoch:  2\n",
      "Accuracy : 0.98375\n",
      "Epoch:  3\n",
      "Accuracy : 0.97625\n",
      "Epoch:  4\n",
      "Accuracy : 0.98\n",
      "Epoch:  5\n",
      "Accuracy : 0.9775\n",
      "Epoch:  6\n",
      "Accuracy : 0.97875\n",
      "Epoch:  7\n",
      "Accuracy : 0.96\n",
      "Epoch:  8\n",
      "Accuracy : 0.985\n",
      "Epoch:  9\n",
      "Accuracy : 0.98375\n",
      "Finished fine tuning.\n",
      "Finished. Going to fine tune the model a bit more\n",
      "Epoch:  0\n",
      "Accuracy : 0.98875\n",
      "Epoch:  1\n",
      "Accuracy : 0.98125\n",
      "Epoch:  2\n",
      "Accuracy : 0.98125\n",
      "Epoch:  3\n",
      "Accuracy : 0.98625\n",
      "Epoch:  4\n",
      "Accuracy : 0.98375\n",
      "Epoch:  5\n",
      "Accuracy : 0.9825\n",
      "Epoch:  6\n",
      "Accuracy : 0.98375\n",
      "Epoch:  7\n",
      "Accuracy : 0.9825\n",
      "Epoch:  8\n",
      "Accuracy : 0.9825\n",
      "Epoch:  9\n",
      "Accuracy : 0.985\n",
      "Epoch:  10\n",
      "Accuracy : 0.9875\n",
      "Epoch:  11\n",
      "Accuracy : 0.98625\n",
      "Epoch:  12\n",
      "Accuracy : 0.97625\n",
      "Epoch:  13\n",
      "Accuracy : 0.9675\n",
      "Epoch:  14\n",
      "Accuracy : 0.98375\n",
      "Finished fine tuning.\n"
     ]
    }
   ],
   "source": [
    "model = ModifiedVGG16Model()\n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('checkpoint_models/trained_model_state.pt') #load\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model=model.cuda()\n",
    "pruner_obj = PrunningFineTuner_VGG16(\"train\", \"test\", model)\n",
    "pruner_obj.test()\n",
    "pruner_obj.prune(percentage_to_prune=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arora.roh/.conda/envs/pytorch_prune2/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ModifiedVGG16Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'checkpoint_models/pruned_model.pt') \n",
    "#save entire model since this is a custom model which would be lost after kernel stops. \n",
    "# will have to reprune to get it back. not deterministic. \n",
    "#ignore the warning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.98375\n"
     ]
    }
   ],
   "source": [
    "pruned_model=torch.load('checkpoint_models/pruned_model.pt') \n",
    "model = pruned_model.cuda()\n",
    "pruner_obj = PrunningFineTuner_VGG16(\"train\", \"test\", model)\n",
    "pruner_obj.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helpers\n",
    "ipdb.set_trace()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
